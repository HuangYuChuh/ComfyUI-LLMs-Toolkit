import comfy
import folder_paths
import nodes
import aiohttp
import json
import asyncio
from aiohttp import ClientSession, ClientError
from typing import Optional, List, Dict
import time  # æ·»åŠ æ—¶é—´æ¨¡å—
import random  # æ·»åŠ éšæœºæ•°æ¨¡å—
import torch


class OpenAICompatibleLoader:
    """
    Custom node for OpenAI compatible API integration
    """

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "base_url": ("STRING", {"default": "Qwen/é€šä¹‰åƒé—®"}),
                "model": ("STRING", {
                    "default": "",
                    "label": "æ¨¡å‹åç§°",
                    "allow_edit": True
                }),
            },
            "optional": {
                "prep_img": ("STRING", {"default": "", "forceInput": True}),
                "system_prompt": ("STRING", {"default": "ä½ æ˜¯ä¸€ä¸ªAIå¤§æ¨¡å‹", "multiline": True}),
                "prompt": ("STRING", {"multiline": True}),
                "temperature": ("FLOAT", {"default": 0.7, "min": 0.0, "max": 2.0}),
                "max_tokens": ("INT", {"default": 512, "min": 1, "max": 4096}),
                "enable_memory": ("BOOLEAN", {"default": False, "label": "Enable Memory"}),
                "api_key": ("STRING", {"default": ""})
            }
        }

    RETURN_TYPES = ("STRING", "INT", "INT")
    RETURN_NAMES = ("text", "input_tokens", "output_tokens")
    FUNCTION = "generate"
    CATEGORY = "ğŸš¦ComfyUI_LLMs_Toolkit"

    async def async_generate(self, payload: dict, actual_base_url: str, api_key: str):
        try:
            async with ClientSession() as session:
                try:
                    async with session.post(
                        f"{actual_base_url}/chat/completions",
                        headers={
                            "Content-Type": "application/json",
                            "Authorization": f"Bearer {api_key}"
                        },
                        json=payload
                    ) as response:
                        # æ‰“å°å®Œæ•´çš„è¯·æ±‚å†…å®¹ä»¥ä¾¿è°ƒè¯•
                        # Simplified debug log
                        print(f"[DEBUG] Request sent to model: {payload['model']}, temp: {payload['temperature']}, max_tokens: {payload['max_tokens']}")
                        response.raise_for_status()
                        data = await response.json()
                        # ç§»é™¤å†—é•¿çš„APIå“åº”æ—¥å¿—
                        response_content = data['choices'][0]['message']['content']
                        # ç®€åŒ–åçš„å“åº”å†…å®¹æ—¥å¿—
                        print(f"Output: {response_content[:50]}...")  # åªä¿ç•™å‰50ä¸ªå­—ç¬¦
                        return [response_content, data]  # è¿”å›å€¼åŒ…æ‹¬ response_content å’Œ data
                except Exception as e:
                    print(f"[ERROR] è¯·æ±‚å¤±è´¥: {str(e)}")  # æ‰“å°é”™è¯¯ä¿¡æ¯
                    raise
        except ClientError as e:
            raise Exception(f"API request failed: {str(e)}")

    def generate(self, base_url: str, api_key: str, prompt: str,
                 model: str, temperature: float,
                 max_tokens: int, system_prompt: Optional[str] = None, prep_img: Optional[str] = None, enable_memory: bool = False, seed: Optional[int] = None):

        content = []  # åˆå§‹åŒ–å†…å®¹åˆ—è¡¨

        # ç§»é™¤å›¾åƒå‚æ•°ç±»å‹çš„æ—¥å¿—

        if prep_img:
            # æ‰“å° prep_img è°ƒè¯•ä¿¡æ¯
            print(f"[DEBUG] Received prep_img: {prep_img[:50]}...")  # åªä¿ç•™å‰50ä¸ªå­—ç¬¦
            # éªŒè¯ prep_img æ˜¯å¦ä¸ºæœ‰æ•ˆçš„ base64 ç¼–ç å­—ç¬¦ä¸²
            if not prep_img.startswith("data:image"):
                raise ValueError("Processed image must be a valid base64 encoded string")

            # å°† prep_img æ·»åŠ åˆ° content ä¸­
            content.append({"type": "image_url", "image_url": {"url": prep_img}})



        if prompt.strip():
            content.append({"type": "text", "text": prompt})

        messages = [] # åˆå§‹åŒ– messages åˆ—è¡¨
        if system_prompt:
            messages.append({
                "role": "system",
                "content": system_prompt
            })

        if content: # åªæœ‰å½“ content åˆ—è¡¨ä¸ä¸ºç©ºæ—¶æ‰æ·»åŠ åˆ° messages
            messages.append({
                "role": "user",
                "content": content
            })
            # ç§»é™¤å¸¦æœ‰å›¾åƒå†…å®¹çš„æ¶ˆæ¯æ—¥å¿—
        elif not prompt.strip() and not system_prompt and prep_img is None:
            raise ValueError("ç”¨æˆ·è¾“å…¥çš„ prompt ä¸èƒ½ä¸ºç©º")

        # æ¨¡å‹é€‰æ‹©é€»è¾‘ (ä¿æŒä¸å˜)
        selected_model = model if model else "glm-4"  # é»˜è®¤æ¨¡å‹ä¸º glm-4
        print(f"[INFO] ä½¿ç”¨æ¨¡å‹: {selected_model}")

        # base_url æ˜ å°„è¡¨ (ä¿æŒä¸å˜)
        base_url_mapping = {
            "Qwen/é€šä¹‰åƒé—®": "https://dashscope.aliyuncs.com/compatible-mode/v1",
            "DeepSeek/æ·±åº¦æ±‚ç´¢": "https://api.deepseek.com/v1/",
            "DouBao/è±†åŒ…": "https://ark.cn-beijing.volces.com/api/v3/",
            "Spark/æ˜Ÿç«": "https://spark-api-open.xf-yun.com/v1/",
            "GLM/æ™ºè°±æ¸…è¨€": "https://open.bigmodel.cn/api/paas/v4/",
            "Moonshot/æœˆä¹‹æš—é¢": "https://api.moonshot.cn/v1",
            "Baichuan/ç™¾å·": "https://api.baichuan-ai.com/v1/",
            "MiniMax/MiniMax": "https://api.minimax.chat/v1/",
            "StepFun/é˜¶è·ƒæ˜Ÿè¾°": "https://api.stepfun.com/v1/",
            "SenseChat/æ—¥æ—¥æ–°": "https://api.sensenova.cn/compatible-mode/v1"
        }
        actual_base_url = base_url_mapping.get(base_url.strip(), base_url)

        # å¯¹è¯å†å²å’Œ payload æ„å»º (ä¿æŒä¸å˜)
        if not enable_memory:
            self._conversation_history = []

        if not hasattr(self, "_conversation_history"):
            self._conversation_history = []

        if system_prompt and not any(msg["role"] == "system" for msg in self._conversation_history):
            self._conversation_history.append({"role": "system", "content": system_prompt})

        # é¿å…é‡å¤æ·»åŠ å†…å®¹
        # ä¿®æ”¹å¯¹è¯å†å²æ·»åŠ é€»è¾‘
        if enable_memory:
            if not any(msg["role"] == "user" and msg["content"] == content for msg in self._conversation_history):
                self._conversation_history.append({"role": "user", "content": content})
        else:
            # å½“ç¦ç”¨è®°å¿†æ—¶ï¼Œæ€»æ˜¯åˆ›å»ºæ–°çš„å¯¹è¯å†å²
            self._conversation_history = [{"role": "user", "content": content}]
        # é‡æ„æ¶ˆæ¯æ ¼å¼åŒ–é€»è¾‘
        if enable_memory:
            # ä½¿ç”¨å¯¹è¯å†å²ç”Ÿæˆæ¶ˆæ¯
            formatted_messages = []
            for msg in self._conversation_history:
                if isinstance(msg["content"], list):
                    formatted_messages.append({
                        "role": msg["role"],
                        "content": msg["content"]
                    })
                else:
                    formatted_messages.append(msg)
        else:
            # å½“ç¦ç”¨è®°å¿†æ—¶ï¼Œç›´æ¥ä½¿ç”¨å½“å‰æ¶ˆæ¯
            formatted_messages = [
                {
                    "role": "user",
                    "content": content
                }
            ]
        # æ·»åŠ æ—¶é—´æˆ³ç¡®ä¿è¯·æ±‚å”¯ä¸€æ€§
        # å¤„ç†éšæœºç§å­
        seed_value = seed if seed is not None else random.randint(1, 1000000)
        
        payload = {
            "model": selected_model,
            "messages": formatted_messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
            "request_id": f"req-{int(time.time() * 1000)}-{hash(str(content))}",  # å”¯ä¸€è¯·æ±‚ID
            "timestamp": int(time.time() * 1000),  # æ¯«ç§’çº§æ—¶é—´æˆ³
            "seed": seed_value  # æ·»åŠ éšæœºç§å­
        }
        
        if "spark-api-open.xf-yun.com" in actual_base_url or "api.baichuan-ai.com" in actual_base_url or "api.sensenova.cn" in actual_base_url:
            # æ˜Ÿç«å¤§æ¨¡å‹éœ€è¦ content ä¸ºå­—ç¬¦ä¸²
            payload["messages"] = [
                {
                    "role": "system",
                    "content": system_prompt
                },
                {
                    "role": "user",
                    "content": prompt if isinstance(prompt, str) else prompt[0]["text"]
                }
            ]
        else:
            # å…¶ä»–æ¨¡å‹ä¿æŒç°æœ‰ç»“æ„
            payload["messages"] = formatted_messages

        # ç®€åŒ–åçš„è°ƒè¯•æ—¥å¿—
        print(f"[DEBUG] Using model: {selected_model}")
        print(f"[DEBUG] Base URL: {actual_base_url}")

        # ç¡®ä¿ temperature å’Œ max_tokens å‚æ•°ç¬¦åˆèŒƒå›´
        if not isinstance(temperature, float):
            try:
                temperature = float(temperature)
            except ValueError:
                raise ValueError(f"temperature å‚æ•°æ— æ³•è½¬æ¢ä¸ºæµ®ç‚¹æ•°: {temperature}")
            if not (0.0 <= temperature <= 2.0):
                raise ValueError(f"temperature å‚æ•°è¶…å‡ºèŒƒå›´: {temperature}")
            
            try:
                max_tokens = int(max_tokens)
            except ValueError:
                raise ValueError(f"max_tokens å‚æ•°æ— æ³•è½¬æ¢ä¸ºæ•´æ•°: {max_tokens}")
            if not (1 <= max_tokens <= 4096):
                raise ValueError(f"max_tokens å‚æ•°è¶…å‡ºèŒƒå›´: {max_tokens}")

        # ç§»é™¤å®Œæ•´çš„payloadæ—¥å¿—
        # ç®€åŒ–åçš„è°ƒç”¨æ—¥å¿—
        # ä½¿ç”¨æ—¶é—´æˆ³ä»£æ›¿ uuid ç”Ÿæˆå”¯ä¸€æ ‡è¯†ç¬¦
        print(f"[{time.strftime('%Y/%m/%d %H:%M:%S')}] INFO PromptTask {int(time.time())}")
        print(f"Input: {prompt}")
        print(f"HTTP Request: POST {actual_base_url}/chat/completions \"HTTP/1.1 200 OK\"")
        # Token è®¡ç®—é€»è¾‘
        def count_tokens(content):
            return sum(len(str(item).split()) for item in content)

        # æŒ‰å­—ç¬¦åˆ†å‰²
        input_tokens = len(prompt)
        if system_prompt:
            input_tokens += len(system_prompt)
        if prep_img:
            # æ ¹æ® prep_img è®¡ç®— token
            if isinstance(prep_img, str):  # Base64 ç¼–ç çš„å­—ç¬¦ä¸²
                # ä¼°ç®— token æ•°é‡ï¼ˆåŸºäºå­—ç¬¦ä¸²é•¿åº¦ï¼‰
                image_tokens = len(prep_img) // 1000
            else:
                image_tokens = 0  # æœªçŸ¥ç±»å‹ï¼Œé»˜è®¤ä¸º 0
            input_tokens += image_tokens


        try:
            time.sleep(1)
            import asyncio

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            try:
                task = loop.create_task(self.async_generate(payload, actual_base_url, api_key))
                response_content, data = loop.run_until_complete(task)
                # Extract completion_tokens from API response
                completion_tokens = data.get("usage", {}).get("completion_tokens", 0)
                return [response_content, int(input_tokens), int(completion_tokens)]
            except Exception as e:
                print(f"[ERROR] å¼‚æ­¥ä»»åŠ¡å¤±è´¥: {str(e)}")
                raise
            finally:
                try:
                    if hasattr(loop, 'shutdown_asyncgens'):
                        loop.run_until_complete(loop.shutdown_asyncgens())
                    # Ensure all tasks are done before closing the loop
                    pending = asyncio.all_tasks(loop)
                    if pending:
                        print(f"[WARNING] There are {len(pending)} pending tasks. Waiting for them to complete...")
                        loop.run_until_complete(asyncio.gather(*pending))
                finally:
                    loop.close()
        except Exception as e:
            raise Exception(f"è¯·æ±‚å¤±è´¥: {str(e)}")


# æ³¨å†ŒèŠ‚ç‚¹ (ä¿æŒä¸å˜)
NODE_CLASS_MAPPINGS = {"OpenAICompatibleLoader": OpenAICompatibleLoader}
NODE_DISPLAY_NAME_MAPPINGS = {"OpenAICompatibleLoader": "OpenAI Compatible Adapter"}

WEB_DIRECTORY = "./web"
__all__ = ['NODE_CLASS_MAPPINGS', 'NODE_DISPLAY_NAME_MAPPINGS']
